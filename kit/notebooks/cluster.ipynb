{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kmedoids\n",
    "import numpy\n",
    "import time\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import pandas as pd\n",
    "from new_modeling_toolkit.common.load_component import Load\n",
    "import pathlib\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from new_modeling_toolkit.visualization import e3_plotly\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "logger.remove()\n",
    "# Set stdout logging level\n",
    "logger.add(sys.__stdout__, level=\"INFO\")\n",
    "logger.add(sys.stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_modeling_toolkit.core.utils.util import DirStructure\n",
    "from new_modeling_toolkit.common import system\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "case = \"test-irp-0622-37days\"\n",
    "\n",
    "dir_str = DirStructure(data_folder=r\"F:\\cpuc-irp.nmt\\data\")\n",
    "dir_str.make_resolve_dir(resolve_settings_name=case)\n",
    "\n",
    "scenarios = pd.read_csv(dir_str.resolve_settings_dir / \"scenarios.csv\")[\"scenarios\"].tolist()\n",
    "\n",
    "_, system = system.System.from_csv(\n",
    "    filename=dir_str.data_interim_dir / \"systems\" / \"test-06-29\" / \"attributes.csv\",\n",
    "    scenarios=scenarios,\n",
    "    data={\"dir_str\": dir_str, \"model_name\": \"resolve\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_timestamps = pd.date_range(\n",
    "    start=system.resources[\"CAISO_Hydro\"].daily_budget.data.index[0], \n",
    "    end=system.resources[\"CAISO_Hydro\"].daily_budget.data.index[-1] + pd.tseries.frequencies.to_offset(\"1D\"), \n",
    "    freq=\"1H\",\n",
    "    inclusive=\"left\",\n",
    ")\n",
    "system.resources[\"CAISO_Hydro\"].daily_budget.data = pd.Series(system.resources[\"CAISO_Hydro\"].daily_budget.data, index=hourly_timestamps).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional\n",
    "from new_modeling_toolkit.core.component import Component\n",
    "from new_modeling_toolkit.core.custom_model import CustomModel\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from pydantic import Field\n",
    "from new_modeling_toolkit.core.utils.core_utils import timer\n",
    "\n",
    "class Clusterer(CustomModel):\n",
    "    components_to_consider: list[tuple[Component, float, str]]\n",
    "    weather_years_to_use: list[int]\n",
    "    rep_period_length: str = Field(\"1D\", description=\"See https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases for valid options (though special offsets like \\\"business days\\\" will likely not work).\")\n",
    "\n",
    "    # Intermediate things\n",
    "    chrono_periods: Optional[pd.DataFrame] = None\n",
    "    distance_matrix: Optional[np.ndarray] = None\n",
    "    clustered_dates: Optional[pd.DataFrame] = None\n",
    "    rmse: Optional[pd.DataFrame] = None\n",
    "    medoid_results: Any = None\n",
    "\n",
    "    # Attributes to pass to kmedoids package\n",
    "    clusters: int\n",
    "    random_state: int = 1730482\n",
    "\n",
    "    def _pivot_chrono_periods(self):\n",
    "\n",
    "        self.chrono_periods = pd.concat(\n",
    "            [\n",
    "                pd.pivot_table(\n",
    "                    multiplier * getattr(component, attr).data.to_frame(), \n",
    "                    index=getattr(component, attr).data.index.date, \n",
    "                    columns=getattr(component, attr).data.index.hour\n",
    "                )\n",
    "                for component, multiplier, attr in self.components_to_consider\n",
    "            ],\n",
    "            axis=1   \n",
    "        )\n",
    "        self.chrono_periods = self.chrono_periods.dropna(how=\"any\").T.reset_index(drop=True).T\n",
    "\n",
    "    @timer\n",
    "    def get_clusters(self):\n",
    "        self._pivot_chrono_periods()\n",
    "\n",
    "        self.distance_matrix = euclidean_distances(self.chrono_periods)\n",
    "\n",
    "        # Add this to attrs above\n",
    "        self.medoid_results = kmedoids.fasterpam(self.distance_matrix, medoids=self.clusters, random_state=self.random_state, n_cpu=4)\n",
    "\n",
    "\n",
    "        # Map chrono and rep periods\n",
    "        medoids = pd.Series(self.medoid_results.medoids).map(self.chrono_periods.reset_index()[\"index\"])\n",
    "\n",
    "        clustered_dates = pd.Series(self.medoid_results.labels, index=self.chrono_periods.index).map(medoids)\n",
    "        clustered_dates.index = pd.to_datetime(clustered_dates.index, infer_datetime_format=True)\n",
    "        clustered_dates = pd.to_datetime(clustered_dates, infer_datetime_format=True) \n",
    "\n",
    "        # Create a new DateTimeIndex that has all the hours\n",
    "        hourly_timestamps = pd.date_range(\n",
    "            start=clustered_dates.index[0], \n",
    "            end=clustered_dates.index[-1] + pd.tseries.frequencies.to_offset(self.rep_period_length), \n",
    "            freq=\"1H\",\n",
    "            inclusive=\"left\",\n",
    "        )\n",
    "\n",
    "        clustered_dates = pd.Series(hourly_timestamps, index=hourly_timestamps).map(clustered_dates).ffill()\n",
    "        clustered_dates = clustered_dates + pd.to_timedelta(clustered_dates.index.hour, unit=\"H\")\n",
    "        \n",
    "        self.clustered_dates = clustered_dates\n",
    "        \n",
    "        return self.clustered_dates\n",
    "\n",
    "    @timer\n",
    "    def calculate_rmse(self):\n",
    "        \"\"\"Calculate RMSE for every component included in Clusterer.\"\"\"\n",
    "        rmse: dict = {}\n",
    "        for component, _, attr in self.components_to_consider:\n",
    "\n",
    "                profiles_for_plotting = pd.concat([getattr(component, attr).data, self.clustered_dates.map(getattr(component, attr).data)], axis=1)\n",
    "                profiles_for_plotting.columns = [\"original\", \"clustered\"]\n",
    "\n",
    "                rmse[component.name] = (((profiles_for_plotting[\"original\"] - profiles_for_plotting[\"clustered\"]) /profiles_for_plotting[\"original\"].max() ) ** 2).mean() ** .5\n",
    "\n",
    "        rmse[\"total\"] = sum(\n",
    "             multiplier * rmse[component.name]\n",
    "             for component, multiplier, _ in self.components_to_consider\n",
    "        )\n",
    "        \n",
    "        self.rmse = pd.Series(rmse)\n",
    "\n",
    "\n",
    "    @timer\n",
    "    def compare_clustered_timeseries(self):\n",
    "        \"\"\"Create a plotly figure with comparison metrics between original and clustered timeseries.\n",
    "        \n",
    "        TODO: Can add more components than just the ones that were used to cluster on to do this comparison\n",
    "        TODO: This won't work out of the box with modeled year timeseries.\n",
    "        \"\"\"\n",
    "        f = open(\"clustering_results.html\",\"w\")\n",
    "        f.close()\n",
    "        # print(components_to_consider)\n",
    "        with open(\"clustering_results.html\", 'a') as f:\n",
    "\n",
    "            for component, _, attr in self.components_to_consider:\n",
    "\n",
    "                profiles_for_plotting = pd.concat([getattr(component, attr).data, self.clustered_dates.map(getattr(component, attr).data)], axis=1)\n",
    "                profiles_for_plotting.columns = [\"original\", \"clustered\"]\n",
    "\n",
    "                fig = make_subplots(\n",
    "                    rows=2, \n",
    "                    cols=3,\n",
    "                    specs=[\n",
    "                        [{\"colspan\": 2, \"type\": \"xy\"}, None, {\"rowspan\": 2, \"type\": \"table\"}],\n",
    "                        [{\"type\": \"xy\"}, {\"type\": \"xy\"}, None],\n",
    "                    ],\n",
    "                    column_widths=[0.35, 0.35, 0.3],\n",
    "                )\n",
    "\n",
    "                # Chronological\n",
    "                fig.add_traces(\n",
    "                    [\n",
    "                        go.Scatter(\n",
    "                            x=profiles_for_plotting.index,\n",
    "                            y=profiles_for_plotting[\"original\"] / profiles_for_plotting[\"original\"].max(),\n",
    "                            marker_color=\"rgb(3, 78, 110)\",\n",
    "                            opacity=0.75,\n",
    "                            name=\"original\",\n",
    "                            showlegend=False,\n",
    "                        ),\n",
    "                        go.Scatter(\n",
    "                            x=profiles_for_plotting.index,\n",
    "                            y=profiles_for_plotting[\"clustered\"] / profiles_for_plotting[\"original\"].max(),\n",
    "                            marker_color=\"rgb(255, 135, 0)\",\n",
    "                            name=\"clustered\",\n",
    "                            opacity=0.75,\n",
    "                            showlegend=False,\n",
    "                        )\n",
    "                    ],\n",
    "                    rows=1,\n",
    "                    cols=1,\n",
    "                )\n",
    "\n",
    "                # Duration curve\n",
    "                fig.add_traces(\n",
    "                    [\n",
    "                        go.Scatter(\n",
    "                            x=profiles_for_plotting.reset_index().index,\n",
    "                            y=profiles_for_plotting[\"original\"].sort_values(ascending=False) / profiles_for_plotting[\"original\"].max(),\n",
    "                            marker_color=\"rgb(3, 78, 110)\",\n",
    "                            opacity=0.75,\n",
    "                            name=\"original\",\n",
    "                            showlegend=False,\n",
    "                        ),\n",
    "                        go.Scatter(\n",
    "                            x=profiles_for_plotting.reset_index().index,\n",
    "                            y=profiles_for_plotting[\"clustered\"].sort_values(ascending=False) / profiles_for_plotting[\"original\"].max(),\n",
    "                            marker_color=\"rgb(255, 135, 0)\",\n",
    "                            name=\"clustered\",\n",
    "                            opacity=0.75,\n",
    "                            showlegend=False,\n",
    "                        )\n",
    "                    ],\n",
    "                    rows=2,\n",
    "                    cols=1,\n",
    "                )\n",
    "\n",
    "                # Histogram\n",
    "                fig.add_traces(\n",
    "                    [\n",
    "                        go.Histogram(\n",
    "                            x=profiles_for_plotting[\"original\"],\n",
    "                            marker_color=\"rgb(3, 78, 110)\",\n",
    "                            opacity=0.75,\n",
    "                            histnorm=\"probability density\",\n",
    "                            name=\"Original\",\n",
    "                        ),\n",
    "                        go.Histogram(\n",
    "                            x=profiles_for_plotting[\"clustered\"],\n",
    "                            marker_color=\"rgb(255, 135, 0)\",\n",
    "                            opacity=0.75,\n",
    "                            histnorm=\"probability density\",\n",
    "                            name=\"Clustered\",\n",
    "                        )\n",
    "                    ],\n",
    "                    rows=2,\n",
    "                    cols=2,\n",
    "                )\n",
    "\n",
    "                # Summary stats table\n",
    "                annual_metrics = profiles_for_plotting.groupby(profiles_for_plotting.index.year)\n",
    "\n",
    "                fig.add_trace(\n",
    "                    go.Table(\n",
    "                        header=dict(\n",
    "                            values=[(\"\", \"\"), (\"<b>Peak</b>\", \"Original\"), (\"<b>Peak</b>\", \"Clustered\"), (\"<b>Mean</b>\", \"Original\"), (\"<b>Mean</b>\", \"Clustered\")],\n",
    "                            font=dict(size=12),\n",
    "                        ),\n",
    "                        cells=dict(\n",
    "                            values=[\n",
    "                                annual_metrics.max().index,\n",
    "                                annual_metrics.max()[\"original\"],\n",
    "                                annual_metrics.max()[\"clustered\"],\n",
    "                                annual_metrics.mean()[\"original\"],\n",
    "                                annual_metrics.mean()[\"clustered\"],\n",
    "                            ],\n",
    "                            font=dict(size=12),\n",
    "                            format=[None, \",.5r\", \",.5r\", \",.5r\", \",.5r\"],\n",
    "                        ),\n",
    "                    ),\n",
    "                    row=1,\n",
    "                    col=3,\n",
    "                )\n",
    "\n",
    "                fig.update_layout(\n",
    "                    height=5 * 144,\n",
    "                    width=12.32 * 144,\n",
    "                    barmode=\"overlay\",\n",
    "                    title=dict(\n",
    "                        text=f\"<b>{component.name}.</b>{attr}\",\n",
    "                        x=0.04,\n",
    "                        y=0.96,\n",
    "                    ),\n",
    "                )\n",
    "                logger.info(f\"Saving timeseries comparison to: {component.name}.{attr}\")\n",
    "                f.write(\n",
    "                    fig.to_html(\n",
    "                        f\"{component.name}-{attr}.html\",\n",
    "                        full_html=False,\n",
    "                        include_plotlyjs=\"cdn\",\n",
    "                    ),\n",
    "                )\n",
    "            fig.show()\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Clusterer(\n",
    "    name=\"test\",\n",
    "    components_to_consider=[\n",
    "        (system.loads[\"CAISO Baseline\"], 3, \"profile\"),\n",
    "    ],\n",
    "    weather_years_to_use=[],\n",
    "    rep_period_length=\"1D\",\n",
    "    clusters=36,\n",
    ")\n",
    "\n",
    "c.get_clusters()\n",
    "# c.compare_clustered_timeseries()\n",
    "c.calculate_rmse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.compare_clustered_timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_load_components = [\n",
    "    (system.resources[\"Arizona_Solar\"], 0, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Baja_California_Wind\"], 0, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"CAISO_Solar\"], 16000, \"provide_power_potential_profile\"), \n",
    "    (system.resources[\"CAISO_Wind\"], 6700, \"provide_power_potential_profile\"), \n",
    "    (system.resources[\"Cape_Mendocino_Offshore_Wind\"], 0, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Central_Valley_North_Los_Banos_Wind\"], 200, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Del_Norte_Offshore_Wind\"], 0, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Diablo_Canyon_Offshore_Wind\"], 1000, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Distributed_Solar\"], 0, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Greater_Imperial_Solar\"], 0, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Greater_Imperial_Wind\"], 0, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Greater_Kramer_Solar\"], 5000, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Greater_LA_Solar\"], 0, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Humboldt_Bay_Offshore_Wind\"], 1000, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Idaho_Wind\"], 0, \"provide_power_potential_profile\"), \n",
    "    (system.resources[\"Morro_Bay_Offshore_Wind\"], 1000, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"New_Mexico_Wind\"], 0, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Northern_California_Solar\"], 0, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Northern_California_Wind\"], 1000, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Riverside_Solar\"], 25000, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Solano_Wind\"], 25, \"provide_power_potential_profile\"), \n",
    "    (system.resources[\"Southern_NV_Eldorado_Solar\"], 8000, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Southern_NV_Eldorado_Wind\"], 500, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Southern_PGAE_Solar\"], 23000, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Tehachapi_Solar\"], 6000, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Tehachapi_Wind\"], 300, \"provide_power_potential_profile\"),\n",
    "    (system.resources[\"Utah_Wind\"], 0, \"provide_power_potential_profile\"), \n",
    "    (system.resources[\"Wyoming_Wind\"], 1500, \"provide_power_potential_profile\"),\n",
    "]\n",
    "\n",
    "gross_load = system.loads[\"CAISO Baseline\"].profile.data.copy(deep=True)\n",
    "gross_load = gross_load * 55000 / gross_load.groupby(gross_load.index.year).max().median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_load_profile = gross_load.copy(deep=True)\n",
    "for (cmp, multiplier, attr) in net_load_components:\n",
    "    net_load_profile = net_load_profile - multiplier * getattr(cmp, attr).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_modeling_toolkit.common.load_component import Load\n",
    "from new_modeling_toolkit.core.temporal.timeseries import NumericTimeseries\n",
    "\n",
    "net_load = Load(name=\"net load\", profile=NumericTimeseries(name=\"net load profile\", data=net_load_profile), profile_model_years=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: To use zonal load instead of each load component (which would also magically make modeled-year timeseries work), seems like we could feed in a zone component to considerand use `get_load()` or something\n",
    "\n",
    "c = Clusterer(\n",
    "    name=\"test\",\n",
    "    components_to_consider=[\n",
    "        (system.loads[\"CAISO Baseline\"], 10 / system.loads[\"CAISO Baseline\"].profile.data.groupby(system.loads[\"CAISO Baseline\"].profile.data).max().median(), \"profile\"),\n",
    "        (net_load, 10 / net_load.profile.data.groupby(net_load.profile.data).max().median(), \"profile\"), \n",
    "        (system.resources[\"Arizona_Solar\"], 1, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Baja_California_Wind\"], 10, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"CAISO_Hydro\"], 20, \"daily_budget\"), \n",
    "        (system.resources[\"CAISO_Solar\"], 1, \"provide_power_potential_profile\"), \n",
    "        (system.resources[\"CAISO_Wind\"], 20, \"provide_power_potential_profile\"), \n",
    "        (system.resources[\"Cape_Mendocino_Offshore_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Central_Valley_North_Los_Banos_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Del_Norte_Offshore_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Diablo_Canyon_Offshore_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Distributed_Solar\"], 1, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Greater_Imperial_Solar\"], 1, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Greater_Imperial_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Greater_Kramer_Solar\"], 1, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Greater_LA_Solar\"], 1, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Humboldt_Bay_Offshore_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Idaho_Wind\"], 20, \"provide_power_potential_profile\"), \n",
    "        (system.resources[\"Morro_Bay_Offshore_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"New_Mexico_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Northern_California_Solar\"], 1, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Northern_California_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Riverside_Solar\"], 1, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Solano_Wind\"], 20, \"provide_power_potential_profile\"), \n",
    "        (system.resources[\"Southern_NV_Eldorado_Solar\"], 1, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Southern_NV_Eldorado_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Southern_PGAE_Solar\"], 1, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Tehachapi_Solar\"], 1, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Tehachapi_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "        (system.resources[\"Utah_Wind\"], 20, \"provide_power_potential_profile\"), \n",
    "        (system.resources[\"Wyoming_Wind\"], 20, \"provide_power_potential_profile\"),\n",
    "    ],\n",
    "    weather_years_to_use=[],\n",
    "    rep_period_length=\"1D\",\n",
    "    clusters=36,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.get_clusters()\n",
    "c.calculate_rmse()\n",
    "c.rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(c.medoid_results.labels).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.clustered_dates.to_csv(\"clustered_dates.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.chrono_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.compare_clustered_timeseries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = c.clustered_dates.resample(\"D\").first().reset_index()\n",
    "df.columns = [\"Chrono Period\", \"Rep Period\"]\n",
    "\n",
    "n_colors = 365\n",
    "colors = px.colors.sample_colorscale(\"IceFire\", [n/(n_colors -1) for n in range(n_colors)])\n",
    "colors = pd.Series(colors, index=range(0, 365))\n",
    "colors = colors.to_frame(name=\"colors\")\n",
    "colors = pd.concat([colors] * 23, axis=0, ignore_index=True)\n",
    "colors.index = pd.date_range(start=\"1/1/1998\", freq=\"D\", end=\"12/31/2020\")[:-6]\n",
    "colors\n",
    "\n",
    "df = pd.merge(df, colors, left_on=\"Chrono Period\", right_index=True)\n",
    "df[\"Month\"] = df[\"Rep Period\"].dt.month\n",
    "df[\"Day\"] = df[\"Rep Period\"].dt.day\n",
    "df = df.sort_values([\"Month\", \"Day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "data=go.Scatter(\n",
    "    x=df[\"Chrono Period\"],\n",
    "    y=df[\"Rep Period\"].dt.strftime(\"%m/%d/%Y\"),\n",
    "    mode=\"markers\",\n",
    "    marker=dict(\n",
    "        size=6,\n",
    "        color=df[\"colors\"],\n",
    "    )\n",
    ")\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "yaxis=dict(\n",
    "    autorange=\"reversed\", \n",
    "    type=\"category\",\n",
    "    dtick=1,\n",
    "    title=\"<b>Sampled Operational Days</b>\",\n",
    "),\n",
    "xaxis=dict(\n",
    "    # autorange=\"reversed\",\n",
    "    title=\"<b>Original Date</b>\",\n",
    "    showgrid=True,\n",
    "    dtick=\"M12\"\n",
    "),\n",
    "height=6*144,\n",
    "width=16*144,\n",
    "margin=dict(\n",
    "    l=120,\n",
    "    b=160,\n",
    "),\n",
    "font=dict(size=10),\n",
    ")\n",
    "fig.show()\n",
    "# fig.write_image(f\"rep-periods.svg\")\n",
    "# fig.write_html(\"rep-periods.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
